{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Markdown\n",
    "\n",
    "Using `MarkdownNodeParser` for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.readers.file import FlatReader\n",
    "from pathlib import Path\n",
    "\n",
    "md_docs = FlatReader().load_data(Path(\"./data/optisol_info.md\"))\n",
    "\n",
    "parser = MarkdownNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(md_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion with Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "def create_index(nodes, colection_name):\n",
    "    vector_store = QdrantVectorStore(colection_name, client=client)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index(nodes, \"optisol_first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index to Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "vector_store = QdrantVectorStore(\"optisol_first\", client=client)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7fe501489160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever()\n",
    "returned_nodes = retriever.retrieve(\"Who is the current Chief Marketing Officer (CMO) of OptiSol Technologies, and what is their background?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in returned_nodes:\n",
    "    print(node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To ChatEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# build index\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,    \n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_synthesizer:text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x7fe507844b80>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))]),\n",
       " 'response_synthesizer:refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x7fe507844b80>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x7fe507844b80>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))]),\n",
       " 'refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x7fe507844b80>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine._response_synthesizer.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current Chief Marketing Officer (CMO) of OptiSol Technologies is Emma Brown. Her background is in digital marketing and brand management.\n"
     ]
    }
   ],
   "source": [
    "# query\n",
    "response = query_engine.query(\"Who is the current Chief Marketing Officer (CMO) of OptiSol Technologies, and what is their background?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rag_nodes(source_nodes):\n",
    "    for node in source_nodes:\n",
    "        print(node.text)\n",
    "\n",
    "def get_rag_response(query_response):\n",
    "    return query_response.response\n",
    "\n",
    "def get_rag_nodes(query_response):\n",
    "    return query_response.source_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change LLM\n",
    "\n",
    "Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.core import Settings\n",
    "\n",
    "# Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "# define LLM\n",
    "llm_mini = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "# build index\n",
    "# index_mini = KeywordTableIndex.from_documents(documents, llm=llm)\n",
    "\n",
    "vector_store = QdrantVectorStore(\"optisol_first\", client=client)\n",
    "index_mini = VectorStoreIndex.from_vector_store(vector_store, llm=llm_mini)\n",
    "\n",
    "# get response from query\n",
    "query_engine_mini = index_mini.as_query_engine(response_mode=\"tree_summarize\")\n",
    "# response = query_engine.query(\n",
    "#     \"What did the author do after his time at Y Combinator?\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "\"Who is the current Chief Marketing Officer (CMO) of OptiSol Technologies, and what is their background?\",\n",
    "\"Describe the specific initiatives OptiSol Technologies has implemented as part of their Green IT Initiative.\",\n",
    "\"What are the key components of OptiSol Technologies' incident response approach in the event of a cybersecurity breach?\",\n",
    "\"List and describe three major partnerships OptiSol Technologies has established for their cloud services.\",\n",
    "\"What are the primary focus areas for OptiSol Technologies' future R&D efforts, and how do these areas align with current industry trends?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current Chief Marketing Officer (CMO) of OptiSol Technologies is Emma Brown. Her background is in digital marketing and brand management, where she drives the company's marketing strategy and brand positioning.\n",
      "=============================================\n",
      "OptiSol Technologies has implemented initiatives such as reducing the environmental impact of technology operations as part of their Green IT Initiative.\n",
      "=============================================\n",
      "The key components of OptiSol Technologies' incident response approach in the event of a cybersecurity breach include identification, containment, eradication, recovery, and post-incident analysis.\n",
      "=============================================\n",
      "OptiSol Technologies has established partnerships with Microsoft for cloud services and AI solutions, Amazon Web Services (AWS) for cloud infrastructure and services, and Google Cloud for AI and machine learning solutions.\n",
      "=============================================\n",
      "The primary focus areas for OptiSol Technologies' future R&D efforts are Quantum Computing, Blockchain Technology, and Edge Computing. These areas align with current industry trends as they represent cutting-edge technologies that have the potential to revolutionize various sectors. Quantum Computing is gaining attention for its ability to solve complex problems at a much faster rate than traditional computers. Blockchain Technology is being explored for its applications in ensuring secure and transparent transactions. Edge Computing is becoming increasingly important as it enables real-time processing by bringing computing power closer to data sources. By focusing on these areas, OptiSol Technologies is positioning itself at the forefront of technological innovation in line with current industry trends.\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "for q in questions:\n",
    "    response = query_engine_mini.query(q)\n",
    "    print(get_rag_response(response))\n",
    "    print(\"===\"*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# reset\n",
    "# query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "\n",
    "# shakespeare!\n",
    "new_summary_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"Remain descriptive.\"\n",
    "    \"But when listing anything, use good formatting.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)\n",
    "\n",
    "query_engine_mini.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": new_summary_tmpl}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt update worked only when I initialized like this (otherwise it ignored the update prompt):\n",
    "\n",
    "`query_engine_mini = index_mini.as_query_engine(response_mode=\"tree_summarize\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current Chief Marketing Officer (CMO) of OptiSol Technologies is Emma Brown. Emma has a background in digital marketing and brand management, and she oversees all marketing and communications efforts at OptiSol. She drives the company's marketing strategy and brand positioning.\n",
      "=============================================\n",
      "As part of their Green IT Initiative, OptiSol Technologies has implemented the following specific initiatives:\n",
      "\n",
      "- Reducing the environmental impact of technology operations.\n",
      "- Implementing sustainable practices in all operations.\n",
      "=============================================\n",
      "The key components of OptiSol Technologies' incident response approach in the event of a cybersecurity breach include:\n",
      "\n",
      "1. Identification\n",
      "2. Containment\n",
      "3. Eradication\n",
      "4. Recovery\n",
      "5. Post-incident analysis\n",
      "\n",
      "Additionally, OptiSol Technologies also provides incident response planning and training to help businesses prepare for and respond to future incidents effectively.\n",
      "=============================================\n",
      "1. **Technology Partnerships:**  \n",
      "   - **Microsoft:** OptiSol Technologies has a strategic partnership with Microsoft for cloud services and AI solutions. This collaboration allows OptiSol to leverage Microsoft's expertise and resources in cloud technology to enhance their own cloud services offerings.\n",
      "   - **Amazon Web Services (AWS):** OptiSol collaborates with AWS for cloud infrastructure and services. By partnering with AWS, OptiSol can provide their clients with access to a wide range of cloud solutions and resources offered by one of the leading cloud service providers in the industry.\n",
      "   - **Google Cloud:** OptiSol has a partnership with Google Cloud for AI and machine learning solutions. This partnership enables OptiSol to incorporate Google Cloud's advanced AI and machine learning capabilities into their cloud services, allowing clients to benefit from cutting-edge technology in their cloud environments.\n",
      "=============================================\n",
      "Primary Focus Areas for OptiSol Technologies' Future R&D Efforts:\n",
      "\n",
      "1. Quantum Computing: Exploring the potential of quantum computing for solving complex problems.\n",
      "2. Blockchain Technology: Investigating blockchain applications for secure and transparent transactions.\n",
      "3. Edge Computing: Developing solutions to bring computing power closer to data sources for real-time processing.\n",
      "\n",
      "These focus areas align with current industry trends as they are all cutting-edge technologies that have the potential to revolutionize various industries. Quantum computing is gaining traction for its ability to solve complex problems at a much faster rate than traditional computers. Blockchain technology is being increasingly adopted for its secure and transparent nature, especially in industries like finance and supply chain. Edge computing is becoming more popular as the need for real-time data processing grows, especially with the rise of IoT devices. OptiSol Technologies' focus on these areas shows their commitment to staying at the forefront of technological advancements and meeting the evolving needs of their clients and the market.\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "for q in questions:\n",
    "    response = query_engine_mini.query(q)\n",
    "    print(get_rag_response(response))\n",
    "    print(\"===\"*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_synthesizer:summary_template': PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, When listing anything, use bullet points.\\nQuery: {query_str}\\nAnswer: ')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine_mini.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.query(\"Who is the current Chief Marketing Officer (CMO) of OptiSol Technologies, and what is their background?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response.source_nodes), response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_nodes = response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in source_nodes:\n",
    "    print(node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2response = chat_engine.query(\"Describe the specific initiatives OptiSol Technologies has implemented as part of their Green IT Initiative.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rag_response(q2response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_rag_nodes(get_rag_nodes(q2response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Rewrite\n",
    "\n",
    "If I decide to change the prompt, I can go here:\n",
    "\n",
    "- [Source](https://docs.llamaindex.ai/en/stable/examples/prompts/prompt_mixin/#customize-the-prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
